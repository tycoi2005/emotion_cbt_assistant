{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca45a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mInstall 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import io\n",
    "import threading\n",
    "import time\n",
    "import base64\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. FIX: Patch Keras to handle the 'batch_shape' error\n",
    "# This must be defined BEFORE loading the model\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "class PatchedInputLayer(InputLayer):\n",
    "    \"\"\"Custom InputLayer to ignore Keras 3 batch_shape errors.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'batch_shape' in kwargs:\n",
    "            kwargs['batch_input_shape'] = kwargs.pop('batch_shape')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# Ensure Matplotlib uses a non-GUI backend for thread safety\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Load Models\n",
    "# ==========================================\n",
    "print(\"Loading Models (Face, Audio, Text)...\")\n",
    "\n",
    "# Text Model\n",
    "text_classifier = pipeline(\"text-classification\", model=\"michellejieli/emotion_text_classifier\")\n",
    "\n",
    "# Audio Model\n",
    "audio_classifier = pipeline(\"audio-classification\", model=\"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\")\n",
    "\n",
    "# Face Model Config\n",
    "face_class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "class FacePredictor:\n",
    "    def __init__(self, model_path=\"best_cnn_aug.h5\"):\n",
    "        try:\n",
    "            # FIX: Use custom_objects to inject our PatchedInputLayer\n",
    "            self.model = tf.keras.models.load_model(\n",
    "                model_path,\n",
    "                custom_objects={'InputLayer': PatchedInputLayer},\n",
    "                compile=False\n",
    "            )\n",
    "            self.loaded = True\n",
    "            print(\"‚úÖ Face Model Loaded with Compatibility Patch.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Face Model Loading Failed: {e}\")\n",
    "            self.loaded = False\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    def predict(self, frame_rgb):\n",
    "        if not self.loaded: return frame_rgb, \"N/A\", 0\n",
    "        gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30,30))\n",
    "\n",
    "        label, conf = \"None\", 0\n",
    "        annotated = frame_rgb.copy()\n",
    "\n",
    "        for (x, y, w, h) in faces[:1]:\n",
    "            face_roi = cv2.resize(gray[y:y+h, x:x+w], (48, 48))\n",
    "            inp = face_roi.astype(\"float32\") / 255.0\n",
    "            inp = inp.reshape(1, 48, 48, 1)\n",
    "\n",
    "            # Run prediction\n",
    "            probs = self.model(inp, training=False).numpy()[0]\n",
    "            idx = np.argmax(probs)\n",
    "            label, conf = face_class_names[idx], probs[idx]\n",
    "\n",
    "            cv2.rectangle(annotated, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            cv2.putText(annotated, f\"{label} ({conf:.2f})\", (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "        return annotated, label, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# read GEMINI_API_KEY from ../.env\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "try:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    print(\"‚úÖ Gemini API configured successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Gemini API configuration failed: {e}\")\n",
    "    gemini_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "        if 'generateContent' in m.supported_generation_methods:\n",
    "            print(f\" - {m.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Unified Multimodal Logic\n",
    "# ==========================================\n",
    "class EmotionIntelligenceApp:\n",
    "    def __init__(self):\n",
    "        self.fs = 16000\n",
    "        self.is_running = False\n",
    "        self.face_predictor = FacePredictor()\n",
    "\n",
    "        # State storage\n",
    "        self.cur_face = \"N/A\"\n",
    "        self.cur_audio = \"N/A\"\n",
    "        self.audio_chunks = []\n",
    "        self.chat_history = []\n",
    "        # UI Elements\n",
    "        self.btn_toggle = widgets.Button(description='‚ñ∂Ô∏è Start System', button_style='success', layout={'width': '315px'})\n",
    "\n",
    "        self.status = widgets.HTML(\"<b>Status:</b> Ready\")\n",
    "\n",
    "        self.video_out = widgets.Image(format='jpeg', width=640)\n",
    "        self.audio_out = widgets.Image(format='png', width=640)\n",
    "\n",
    "        self.text_in = widgets.Textarea(placeholder='Type your thoughts here...', layout={'width': '635px', 'height': '70px'})\n",
    "\n",
    "        self.btn_report = widgets.Button(description='üìä Get Multimodal Report', button_style='info', layout={'width': '315px'})\n",
    "        self.btn_chat = widgets.Button(description='üó£Ô∏è Chat with AI', button_style='info', layout={'width': '315px'})\n",
    "        self.report_out = widgets.Output()\n",
    "\n",
    "        # Connections\n",
    "        self.btn_toggle.on_click(self.toggle_process)\n",
    "        self.btn_report.on_click(self.generate_report)\n",
    "        self.btn_chat.on_click(self.chat_with_ai)\n",
    "\n",
    "    def toggle_process(self, _):\n",
    "        if not self.is_running:\n",
    "            self.is_running = True\n",
    "            self.btn_toggle.description = '‚èπÔ∏è Stop System'\n",
    "            self.btn_toggle.button_style = 'danger'\n",
    "            self.audio_chunks = []\n",
    "            self.status.value = \"<b style='color:red;'>üî¥ LIVE MONITORING</b>\"\n",
    "            threading.Thread(target=self._video_thread, daemon=True).start()\n",
    "            threading.Thread(target=self._audio_thread, daemon=True).start()\n",
    "        else:\n",
    "            self.is_running = False\n",
    "            self.btn_toggle.description = '‚ñ∂Ô∏è Start System'\n",
    "            self.btn_toggle.button_style = 'success'\n",
    "            self.status.value = \"<b>Status:</b> System Stopped\"\n",
    "\n",
    "    def _video_thread(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        while self.is_running:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            annotated, label, conf = self.face_predictor.predict(rgb)\n",
    "            self.cur_face = label\n",
    "\n",
    "            _, jpeg = cv2.imencode('.jpg', cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))\n",
    "            self.video_out.value = jpeg.tobytes()\n",
    "            time.sleep(0.03)\n",
    "        cap.release()\n",
    "\n",
    "    def _audio_thread(self):\n",
    "        VOLUME_THRESHOLD = 0.005\n",
    "\n",
    "        with sd.InputStream(samplerate=self.fs, channels=1) as stream:\n",
    "            while self.is_running:\n",
    "                chunk, _ = stream.read(int(self.fs * 0.5))\n",
    "                self.audio_chunks.append(chunk)\n",
    "\n",
    "                # Plot/Analyze last bits\n",
    "                all_y = np.concatenate(self.audio_chunks, axis=0).flatten()\n",
    "                recent_y = all_y[-(self.fs * 3):] # Analysis window\n",
    "\n",
    "                # Calculate the Root Mean Square (average volume)\n",
    "                rms = np.sqrt(np.mean(recent_y**2))\n",
    "                if rms < VOLUME_THRESHOLD:\n",
    "                    # Audio is not audible (silence)\n",
    "                    self.cur_audio = \"neutral\"\n",
    "                    # Still update the waveform so the user sees it's moving\n",
    "                    self._draw_wave(all_y[-(self.fs * 10):], \"neutral (silence)\")\n",
    "                else:\n",
    "                    try:\n",
    "                        res = audio_classifier(recent_y)\n",
    "                        self.cur_audio = res[0]['label']\n",
    "                        self._draw_wave(all_y[-(self.fs * 10):], self.cur_audio) # View window\n",
    "                    except: pass\n",
    "\n",
    "    def _draw_wave(self, y, label):\n",
    "        # Normalize for visibility\n",
    "        peak = np.max(np.abs(y))\n",
    "        y_scaled = y / peak * 0.8 if peak > 0.01 else y\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "        librosa.display.waveshow(y_scaled, sr=self.fs, ax=ax, color='#3498db', alpha=0.6)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5, 0.5, f\"AUDIO: {label.upper()}\", transform=ax.transAxes, color='red',\n",
    "                fontsize=30, fontweight='bold', alpha=0.1, ha='center', va='center')\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', transparent=False)\n",
    "        plt.close(fig)\n",
    "        buf.seek(0)\n",
    "        self.audio_out.value = buf.read()\n",
    "\n",
    "    def generate_report(self, _):\n",
    "        with self.report_out:\n",
    "            self.report_out.clear_output()\n",
    "            txt = self.text_in.value.strip()\n",
    "            text_emo = \"N/A\"\n",
    "            if txt:\n",
    "                t_res = text_classifier(txt)\n",
    "                text_emo = t_res[0]['label'].upper()\n",
    "\n",
    "            display(HTML(f\"\"\"\n",
    "                <div style=\"border-left: 5px solid #3498db; background: #f9f9f9; padding: 15px; border-radius: 5px; width: 620px;\">\n",
    "                    <h3 style=\"margin-top:0;\">Multimodal Emotion Report</h3>\n",
    "                    <b>üì∑ Face:</b> {self.cur_face.upper()}<br>\n",
    "                    <b>üéôÔ∏è Voice:</b> {self.cur_audio.upper()}<br>\n",
    "                    <b>üí¨ Text:</b> {text_emo}\n",
    "                    <hr>\n",
    "                    <small>Timestamp: {time.ctime()}</small>\n",
    "                </div>\n",
    "            \"\"\"))\n",
    "    def chat_with_ai(self, _):\n",
    "        \"\"\"Chat with Gemini AI with emotion context.\"\"\"\n",
    "        with self.report_out:\n",
    "            self.report_out.clear_output()\n",
    "\n",
    "            # Get user input\n",
    "            user_text = self.text_in.value.strip()\n",
    "            if not user_text:\n",
    "                display(HTML(\"<p style='color: red;'>Please enter some text to chat with AI.</p>\"))\n",
    "                return\n",
    "\n",
    "            # Check if Gemini is configured\n",
    "            if gemini_model is None:\n",
    "                display(HTML(\"<p style='color: red;'>‚ùå Gemini API not configured. Please set your API key.</p>\"))\n",
    "                return\n",
    "\n",
    "            # Get current text emotion\n",
    "            text_emotion = \"N/A\"\n",
    "            try:\n",
    "                t_res = text_classifier(user_text)\n",
    "                text_emotion = t_res[0]['label']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Build emotion context\n",
    "            emotion_context = f\"\"\"\n",
    "                ## Current User Emotional State:\n",
    "                - Facial Expression: {self.cur_face}\n",
    "                - Voice Tone: {self.cur_audio}\n",
    "                - Text Sentiment: {text_emotion}\n",
    "                \"\"\"\n",
    "\n",
    "            # Create system prompt with emotion awareness\n",
    "            system_prompt = \"\"\"You are an empathetic AI assistant with access to multimodal emotion detection.\n",
    "                You can see the user's emotional state through their facial expressions, voice tone, and text sentiment.\n",
    "\n",
    "                Your role is to:\n",
    "                1. Acknowledge and validate the user's emotions\n",
    "                2. Provide supportive and appropriate responses based on their emotional state\n",
    "                3. Offer helpful suggestions or coping strategies when appropriate\n",
    "                4. Maintain a warm, understanding, and non-judgmental tone\n",
    "\n",
    "                When the user appears distressed (sad, angry, fearful), be extra compassionate and supportive.\n",
    "                When they appear happy or neutral, be engaging and positive.\n",
    "                Always consider the emotional context in your responses.\"\"\"\n",
    "\n",
    "            # Combine system prompt, emotion context, and user message\n",
    "            full_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "                {emotion_context}\n",
    "\n",
    "                User message: {user_text}\n",
    "\n",
    "                Please respond in a supportive and contextually appropriate way, taking into account their current emotional state.\"\"\"\n",
    "\n",
    "            # Display user message\n",
    "            display(HTML(f\"\"\"\n",
    "                <div style=\"background: #e3f2fd; padding: 12px; border-radius: 8px; margin-bottom: 10px; width: 620px;\">\n",
    "                    <b>üßë You:</b> {user_text}\n",
    "                    <div style=\"font-size: 0.85em; color: #666; margin-top: 5px;\">\n",
    "                        üòä Face: {self.cur_face} | üé§ Voice: {self.cur_audio} | üí≠ Text: {text_emotion}\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"))\n",
    "\n",
    "            # Show loading indicator\n",
    "            display(HTML(\"<p>ü§ñ AI is thinking...</p>\"))\n",
    "\n",
    "            try:\n",
    "                # Call Gemini API\n",
    "                response = gemini_model.generate_content(full_prompt)\n",
    "                ai_response = response.text\n",
    "\n",
    "                # Store in chat history\n",
    "                self.chat_history.append({\n",
    "                    'user': user_text,\n",
    "                    'ai': ai_response,\n",
    "                    'emotions': {\n",
    "                        'face': self.cur_face,\n",
    "                        'audio': self.cur_audio,\n",
    "                        'text': text_emotion\n",
    "                    },\n",
    "                    'timestamp': time.ctime()\n",
    "                })\n",
    "\n",
    "                # Clear and display response\n",
    "                self.report_out.clear_output()\n",
    "\n",
    "                # Display user message again\n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"background: #e3f2fd; padding: 12px; border-radius: 8px; margin-bottom: 10px; width: 620px;\">\n",
    "                        <b>üßë You:</b> {user_text}\n",
    "                        <div style=\"font-size: 0.85em; color: #666; margin-top: 5px;\">\n",
    "                            üòä Face: {self.cur_face} | üé§ Voice: {self.cur_audio} | üí≠ Text: {text_emotion}\n",
    "                        </div>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "                # Display AI response\n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"background: #f1f8e9; padding: 12px; border-radius: 8px; margin-bottom: 10px; width: 620px; border-left: 4px solid #8bc34a;\">\n",
    "                        <b>ü§ñ AI Assistant:</b><br>\n",
    "                        <div style=\"margin-top: 8px; line-height: 1.6;\">{ai_response.replace(chr(10), '<br>')}</div>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "                # Clear text input for next message\n",
    "                self.text_in.value = \"\"\n",
    "\n",
    "            except Exception as e:\n",
    "                self.report_out.clear_output()\n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"background: #ffebee; padding: 12px; border-radius: 8px; width: 620px; border-left: 4px solid #f44336;\">\n",
    "                        <b>‚ùå Error:</b> {str(e)}<br>\n",
    "                        <small>Please check your API key and internet connection.</small>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "# ==========================================\n",
    "# 4. Display Application\n",
    "# ==========================================\n",
    "app = EmotionIntelligenceApp()\n",
    "\n",
    "title = HTML(\"<h2 style='text-align:center;'>Multimodal Emotion Intelligence</h2>\")\n",
    "ui_layout = widgets.VBox([\n",
    "    widgets.HBox([app.btn_toggle], layout={'justify_content': 'center'}),\n",
    "    app.status,\n",
    "    app.video_out,\n",
    "    app.audio_out,\n",
    "    widgets.HTML(\"<b>Analyze Text Sentiment:</b>\"),\n",
    "    app.text_in,\n",
    "    widgets.HBox([app.btn_chat, app.btn_report], layout={'justify_content': 'center'}),\n",
    "    app.report_out\n",
    "], layout={'align_items': 'center', 'width': '100%'})\n",
    "\n",
    "display(title, ui_layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
